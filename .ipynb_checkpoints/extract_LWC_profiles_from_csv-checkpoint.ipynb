{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b618ef21-2fac-4720-b03e-8c32debeedc6",
   "metadata": {},
   "source": [
    "### Code to calculate mean profiles of LWC for different bins of Iorg \n",
    "#### author: Claudia Acquistapace\n",
    "#### date; 08/02/2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35216332-4f2c-4ac0-b4bd-ab63b142897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:      (index: 51000)\n",
      "Coordinates:\n",
      "  * index        (index) int64 0 1 2 3 4 5 ... 50995 50996 50997 50998 50999\n",
      "Data variables:\n",
      "    location     (index) object '/p/scratch/deepacf/kiste/DC/dataset/barbados...\n",
      "    datetime     (index) object '2017-03-19 13:06:10.200' ... '2021-12-31 15:...\n",
      "    day_of_year  (index) int64 78 78 78 78 78 78 78 ... 365 365 365 365 365 365\n",
      "    year         (index) int64 2017 2017 2017 2017 2017 ... 2021 2021 2021 2021\n",
      "    hour         (index) int64 13 13 13 13 13 13 13 13 ... 14 14 14 14 14 14 15\n",
      "    minute       (index) int64 6 6 6 6 6 21 21 21 21 ... 10 20 20 30 30 40 40 20\n",
      "    iorg         (index) float64 0.7773 0.6127 0.5857 ... 0.7671 0.6495 0.7025\n",
      "    tsne-2d-one  (index) float64 0.8025 0.6687 0.475 ... 0.4789 0.2564 0.4766\n",
      "    tsne-2d-two  (index) float64 0.4999 0.4063 0.4236 ... 0.8031 0.3034 0.8062\n",
      "    month        (index) int64 3 3 3 3 3 3 3 3 3 ... 12 12 12 12 12 12 12 12 12\n",
      "['' '' '' ... '' '' '']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from warnings import warn\n",
    "import datetime as dt\n",
    "from scipy import interpolate\n",
    "import matplotlib as mpl\n",
    "import os.path\n",
    "import itertools\n",
    "import os.path\n",
    "\n",
    "lat_lon_path = '/data/sat/goes-r-abi/ATLANTIC/2017-21_experiment/lat_lon_leif/'\n",
    "iorg_data_df = pd.read_csv( '/work/ML_work_DC/claudia_51k_iorg.csv')\n",
    "    \n",
    "iorg_data = iorg_data_df.to_xarray()\n",
    "print(iorg_data)\n",
    "\n",
    "# reading the number of images in the selected file \n",
    "n_images = len(iorg_data.index.values)\n",
    "\n",
    "# path and filename of ERA5 data\n",
    "path_era5 = '/data/mod/era/era5/tropical_atlantic'\n",
    "# reading one era5 data to extract height dimension (needed for defining output matrices of era5 data)\n",
    "profile_era5_start = xr.open_dataset(path_era5+'/2017/03/profb_presslev_20170322T1300.nc')\n",
    "height = profile_era5_start.level.values\n",
    "\n",
    "# output path \n",
    "path_out = '/net/ostro/ML_work_DC/'\n",
    "\n",
    "# initializing output variables\n",
    "IORG = iorg_data.iorg.values\n",
    "clwc_profile = np.zeros((n_images, len(height)))\n",
    "clwc_profile_std = np.zeros((n_images, len(height)))\n",
    "clwc_profile.fill(np.nan)\n",
    "clwc_profile_std.fill(np.nan)\n",
    "im_name_arr = np.asarray([''  for x in np.arange(n_images)])\n",
    "id_lat_lon_arr =  np.asarray([''  for x in np.arange(n_images)])\n",
    "datetime_arr = np.asarray([''  for x in np.arange(n_images)])\n",
    "\n",
    "ind_start = len('/p/scratch/deepacf/kiste/DC/dataset/barbados/barbados_leif/1/')\n",
    "print(im_name_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2ba3f3-d597-44cc-be84-0ab8ebb751b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unexpected encoding parameters for 'netCDF4' backend: ['complevele']. Valid encodings are: {'_FillValue', 'contiguous', 'chunksizes', 'least_significant_digit', 'dtype', 'compression', 'shuffle', 'fletcher32', 'zlib', 'complevel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2709618/1942839201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# storing ncdf data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m crop_data.to_netcdf(path_out+'IORG_LWC_era5.nc', encoding={\"clwc\":{\"zlib\":True, \"complevel\":9}, \\\n\u001b[0m\u001b[1;32m    127\u001b[0m                                                            \u001b[0;34m\"clwc_std\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"zlib\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"complevel\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                                                            \u001b[0;34m\"IORG\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"zlib\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"complevel\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_netcdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1912\u001b[0;31m         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n\u001b[0m\u001b[1;32m   1913\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;31m# TODO: allow this work (setting up the file for writing array data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# to be parallelized with dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m         dump_to_store(\n\u001b[0m\u001b[1;32m   1233\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m     \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         self.set_variables(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mset_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode_variable_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_encoding_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             target, source = self.prepare_variable(\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mprepare_variable\u001b[0;34m(self, name, variable, check_encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m    479\u001b[0m             )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         encoding = _extract_nc4_variable_encoding(\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_invalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_extract_nc4_variable_encoding\u001b[0;34m(variable, raise_on_invalid, lsd_okay, h5py_okay, backend, unlimited_dims)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0minvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_encodings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;34mf\"unexpected encoding parameters for {backend!r} backend: {invalid!r}. Valid \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;34mf\"encodings are: {valid_encodings!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unexpected encoding parameters for 'netCDF4' backend: ['complevele']. Valid encodings are: {'_FillValue', 'contiguous', 'chunksizes', 'least_significant_digit', 'dtype', 'compression', 'shuffle', 'fletcher32', 'zlib', 'complevel'}"
     ]
    }
   ],
   "source": [
    "# loop on the images to collect era 5 data if they are present\n",
    "for ind_images in range(n_images):\n",
    "\n",
    "    # read image id string and id for lat/lon\n",
    "    id_image = iorg_data.location.values[ind_images][ind_start:ind_start+14]\n",
    "    im_name_arr[ind_images] = id_image\n",
    "\n",
    "    id_lat_lon = iorg_data.location.values[ind_images][ind_start+14:ind_start+16]\n",
    "    id_lat_lon_arr[ind_images] = id_lat_lon\n",
    "    \n",
    "    #print(ind_images, id_lat_lon, id_image)\n",
    "    \n",
    "    # reading year, month, day to build name of the image and find corresponding era5 data\n",
    "    datetime_value = pd.to_datetime(iorg_data.datetime.values[ind_images])\n",
    "    datetime_arr[ind_images] = datetime_value\n",
    "\n",
    "    yy = str(datetime_value.year)\n",
    "    mm = str(datetime_value.month)\n",
    "    dd = str(datetime_value.day)\n",
    "    hh = str(datetime_value.hour)\n",
    "    mn = str(datetime_value.minute)\n",
    "\n",
    "    # reading lat/lon\n",
    "    # building era5 path\n",
    "    if len(dd) == 1:\n",
    "        dd = '0'+dd\n",
    "    if len(hh) == 1:\n",
    "        hh = '0'+hh\n",
    "    if len(mm) == 1:\n",
    "        mm = '0'+mm\n",
    "    if len(mn) == 1:\n",
    "        mn = '0'+mn\n",
    "\n",
    "    # constructing date of the selected image (needed for era5)\n",
    "    date = yy+mm+dd+hh+mn+'00'\n",
    "\n",
    "    # assigning path for era5 based on the date\n",
    "    era5_day_path = path_era5+'/'+yy+'/'+mm+'/'\n",
    "\n",
    "    # proceed only if there are era5 data for the selected date\n",
    "    if os.path.isfile(era5_day_path+'profb_presslev_'+yy+mm+dd+'T'+hh+'00.nc') * \\\n",
    "     os.path.isfile(era5_day_path+'surfskinvarb_'+yy+mm+dd+'T'+hh+'00.nc'):\n",
    "\n",
    "        # find how many lat/lons we have associated to the id-image\n",
    "        if id_lat_lon != '_h':\n",
    "            file_lat =  lat_lon_path+id_image+'_lat'+id_lat_lon+'.npy'\n",
    "            file_lon =  lat_lon_path+id_image+'_lon'+id_lat_lon+'.npy'\n",
    "        else:\n",
    "            file_lat =  '/net/ostro/ML_work_DC/halo_lat_lon/halo_lat.npy'\n",
    "            file_lon =  '/net/ostro/ML_work_DC/halo_lat_lon/halo_lon.npy'              \n",
    "        #print(file_lat, file_lon)\n",
    "\n",
    "        # if lat and lon files are present then process the data\n",
    "        if (os.path.isfile(file_lat) * os.path.isfile(file_lon)):\n",
    "\n",
    "            # reading lats/lons for the id_image\n",
    "            lat_data = np.load(file_lat)\n",
    "            lon_data = np.load(file_lon)\n",
    "            lat_max = np.nanmax(lat_data)\n",
    "            lat_min = np.nanmin(lat_data)\n",
    "            lon_min = np.nanmin(lon_data)\n",
    "            lon_max = np.nanmax(lon_data)\n",
    "\n",
    "            # reading era\n",
    "            profile_era5 = xr.open_dataset(era5_day_path+'profb_presslev_'+yy+mm+dd+'T'+hh+'00.nc')\n",
    "            surface_era5 = xr.open_dataset(era5_day_path+'surfskinvarb_'+yy+mm+dd+'T'+hh+'00.nc')\n",
    "\n",
    "            # selecting the area corresponding to the crop\n",
    "            surface_crop = surface_era5.where((surface_era5.latitude > lat_min)*(surface_era5.latitude <= lat_max) \\\n",
    "                                            * (surface_era5.longitude > lon_min) *(surface_era5.longitude <= lon_max))\n",
    "\n",
    "            profiles_crop = profile_era5.where((profile_era5.latitude > lat_min)*(profile_era5.latitude <= lat_max) \\\n",
    "                                            * (profile_era5.longitude > lon_min)*(profile_era5.longitude <= lon_max))\n",
    "            \n",
    "            \n",
    "            # saving mean value of cloud liquid water content\n",
    "            clwc_profile[ind_images,:] = profiles_crop.clwc.mean(dim=('longitude', 'latitude'), skipna='True')\n",
    "            clwc_profile_std[ind_images,:] = profiles_crop.clwc.std(dim=('longitude', 'latitude'), skipna='True')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d49e5e8-965d-4f00-82ff-6dde04ec8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing data in a ncdf file for each area\n",
    "crop_data = xr.Dataset(\n",
    "data_vars={\n",
    "    'im_names': (('n_crops',), im_name_arr, {'long_name': 'Names of image', 'units':''}),\n",
    "    'datetime': (('n_crops',), datetime_arr, {'long_name': 'Names of image', 'units':''}),\n",
    "     \"IORG\": (('n_crops',), IORG, {'long_name': 'organization index', 'units':'', \"standard_name\": \"IORG\"}),\n",
    "    'clwc_std':(('n_crops','levels'), clwc_profile_std, {'long_name': 'specific cloud liquid water content standard deviation', 'units':'kg kg**-1'}),\n",
    "    'clwc':(('n_crops','levels'), clwc_profile, {'long_name': 'specific cloud liquid water content', 'standard_name':'rel hum', 'units':'kg kg**-1'}),\n",
    "},\n",
    "coords={\n",
    "    \"n_crops\": (('n_crops',), np.arange(n_images) ,), # leave units intentionally blank, to be defined in the encoding\n",
    "    \"levels\": (('levels',), height, {\"axis\": \"pressure_level\",\"positive\": \"up\",\"units\": \"millibars\", \"long_name\":'pressure_level'}),\n",
    "},\n",
    "attrs={'CREATED_BY'     : 'Claudia Acquistapace',\n",
    "                'CREATED_ON'       : str(datetime.now()),\n",
    "                'FILL_VALUE'       : 'NaN',\n",
    "                'PI_NAME'          : 'Claudia Acquistapace',\n",
    "                'PI_AFFILIATION'   : 'University of Cologne (UNI), Germany',\n",
    "                'PI_ADDRESS'       : 'Institute for geophysics and meteorology, Pohligstrasse 3, 50969 Koeln',\n",
    "                'PI_MAIL'          : 'cacquist@meteo.uni-koeln.de',\n",
    "                'DATA_DESCRIPTION' : 'ERA5 variables for all the crops of the selected satellite position ',\n",
    "                'DATA_DISCIPLINE'  : 'Atmospheric Physics - Remote Sensing Radar Profiler',\n",
    "                'DATA_GROUP'       : 'Model: reanalysis',\n",
    "                'DATA_LOCATION'    : 'Atlantic Ocean - Eurec4a campaign domain',\n",
    "                'DATA_SOURCE'      : 'ERA5',\n",
    "                'DATA_PROCESSING'  : 'https://github.com/ClauClouds/ML_work_DC',\n",
    "                'COMMENT'          : '' }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# assign additional attributes following CF convention\n",
    "crop_data = crop_data.assign_attrs({\n",
    "    \"Conventions\": \"CF-1.8\",\n",
    "    \"title\": crop_data.attrs[\"DATA_DESCRIPTION\"],\n",
    "    \"institution\": crop_data.attrs[\"PI_AFFILIATION\"],\n",
    "    \"history\": \"\".join([\n",
    "        \"source: \" + crop_data.attrs[\"DATA_SOURCE\"] + \"\\n\",\n",
    "        \"processing: \" + crop_data.attrs[\"DATA_PROCESSING\"] + \"\\n\",\n",
    "        \" adapted to enhance CF compatibility\\n\",\n",
    "    ]),  # the idea of this attribute is that each applied transformation is appended to create something like a log\n",
    "    \"featureType\": \"satellite-era5\",\n",
    "})\n",
    "\n",
    "# storing ncdf data\n",
    "crop_data.to_netcdf(path_out+'IORG_LWC_era5.nc', encoding={\"clwc\":{\"zlib\":True, \"complevel\":9}, \\\n",
    "                                                           \"clwc_std\":{\"zlib\":True, \"complevel\":9}, \\\n",
    "                                                           \"IORG\":{\"zlib\":True, \"complevel\":9}, \\\n",
    "                                                           \"datetime\":{'zlib':True, \"complevel\":9}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d156a-0da6-42b1-91d1-3777720769e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
